{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technicals extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "**Approach:** Sift through technical functions in `ta.py` file, research sensible parameters for each function. Sometimes it seems like multiple runs should be extracted with each function with different parameters, which is denoted by a list of values. \n",
    "\n",
    "### Good to go\n",
    "\n",
    "- 'MA'\n",
    "    - n: [5, 20, 90, 260]\n",
    "- 'STDDEV'\n",
    "    - n: [5, 20, 90, 260]\n",
    "- 'RSI'\n",
    "    - n: [6, 12]\n",
    "- MACD'\n",
    "    - n_fast: 12\n",
    "    - n_slow: 26\n",
    "\n",
    "- 'BBANDS'\n",
    "    - n: [5, 20, 90, 260]\n",
    "\n",
    "- 'MFI' money flow index ratio\n",
    "    - n: 14\n",
    "- 'Chaikin'\n",
    "    - None\n",
    "- 'EMA'\n",
    "    - n: [5, 20, 90, 260]\n",
    "- 'KST'\n",
    "    - r: (10, 10, 10, 15)\n",
    "    - n: (10, 15, 20, 30)\n",
    "    \n",
    "- 'TSI'\n",
    "    - r: 25\n",
    "    - s: 13\n",
    "\n",
    "- 'TRIX'\n",
    "    - n: [5, 20, 90, 260]\n",
    "\n",
    "- 'STOK'\n",
    "    - None\n",
    "\n",
    "- 'STO'\n",
    "    - n: [5, 20, 90, 260]\n",
    "\n",
    "- 'ROC'\n",
    "    - n: [5, 20, 90, 260]\n",
    "\n",
    "- 'PPSR' \n",
    "\n",
    "- 'OBV'\n",
    "    - n: [5, 20, 90, 260]\n",
    "\n",
    "- 'MassI'\n",
    "    - None\n",
    "    \n",
    "- 'MOM'\n",
    "    - n: 1 \n",
    "\n",
    "- 'COPP' \n",
    "    - n: 10\n",
    "\n",
    "- ACCDIST'\n",
    "    - n: 1\n",
    "\n",
    "- 'ADX'\n",
    "    - n: 14\n",
    "    - n_ADX: 50\n",
    "- 'ATR'\n",
    "    - n: 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential implementation\n",
    "\n",
    "- differences on any or all of these columns\n",
    "\n",
    "### Missing end data\n",
    "\n",
    "'ULTOSC'\n",
    "\n",
    "'Vortex'\n",
    "\n",
    "'EOM' ease of movement\n",
    "\n",
    "'KELCH'\n",
    "\n",
    "'DONCH'\n",
    "\n",
    "'CCI' Commodity channel index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/critical_feature_extraction\n"
     ]
    }
   ],
   "source": [
    "cd .. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run __init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load technicals .py file\n",
    "from lib import ta\n",
    "\n",
    "import inspect\n",
    "import string\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hacky way to get a dictionary of all the imported technical functions\n",
    "tech_funcs = dict(filter(lambda x: x[0][0] in string.ascii_uppercase, inspect.getmembers(ta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each tuple contains the *args for a single run\n",
    "#Best guesses from the internet, \"A critical extraction ..\" paper, and the ta.py code\n",
    "\n",
    "grid = {\"MA\": [(5,), (20,)],\n",
    "        \"STDDEV\": [(5,), (20,)],\n",
    "        \"RSI\": [(6,), (12,)],\n",
    "        \"MACD\": [(12, 26)],\n",
    "        \"BBANDS\": [(5,), (20,)],\n",
    "        \"MFI\": [(14,)],\n",
    "        \"Chaikin\": [()],\n",
    "        \"EMA\": [(5,), (20,)],\n",
    "        \"KST\": [(10, 10, 10, 15, 10, 15, 20, 30)],\n",
    "        \"TSI\": [(25, 13)],\n",
    "        \"TRIX\": [(5,), (20,)],\n",
    "        \"STOK\": [()],\n",
    "        \"STO\": [(5,), (20,)],\n",
    "        \"ROC\": [(5,), (20,)],\n",
    "        \"PPSR\": [()],\n",
    "        \"OBV\": [(5,), (20,)],\n",
    "        \"MassI\": [()],\n",
    "        \"MOM\": [(1,)],\n",
    "        \"COPP\": [(10,)],\n",
    "        \"ADX\": [(14, 50)],\n",
    "        \"ATR\": [(14,)],\n",
    "        \"FORCE\": [(2,)],\n",
    "        \"ACCDIST\": [(1,)]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#serially apply every technical function in dictionary to an initial dataframe\n",
    "def extract_technicals(df, tech_funcs, grid):\n",
    "    \n",
    "    output = df\n",
    "    for name, func in tech_funcs.items():\n",
    "        arg_list = grid[name]\n",
    "        for arg_tuple in arg_list:\n",
    "            output = func(output, *arg_tuple)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#serialize technical functions extraction objects\n",
    "tech_func_tools = [tech_funcs, grid]\n",
    "\n",
    "with open(\"lib/tech_func_tools.pkl\", \"wb\") as dump_file:\n",
    "    pickle.dump(tech_func_tools, dump_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract technicals from every individual stock CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab list of csv names in the directory\n",
    "individuals = os.listdir(path = \"data/sandp500/individual_stocks_5yr/\")\n",
    "\n",
    "for csv in individuals:\n",
    "    csv_path = \"data/sandp500/individual_stocks_5yr/\" + csv\n",
    "    df = pd.read_csv(csv_path)\n",
    "    try:\n",
    "        df_technicals = extract_technicals(df, tech_funcs, grid)\n",
    "        df_technicals.to_csv(\"data/sandp500/individual_stocks_5yr_TECHNICALS/\" + csv)\n",
    "    except IndexError:\n",
    "        print(f\"Technical extraction failed on {csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing `extract_technicals` on single stock csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1258, 49)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = pd.read_csv(\"data/sandp500/individual_stocks_5yr/A_data.csv\")\n",
    "sp_technicals = extract_technicals(sp, tech_funcs, grid)\n",
    "sp_technicals.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Drop the Acc/Dist_ROC_1 feature because it is only producing 0's\n",
    "2. Drop all NaN values\n",
    "3. Create a DataFrame with:\n",
    "    - count *before* cleaning\n",
    "    - count *after* cleaning\n",
    "    - *difference* between the original and new count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make an empty dict\n",
    "shape_diff = {}\n",
    "\n",
    "#create a list of the different tickers \n",
    "individuals = os.listdir(path = \"data/sandp500/individual_stocks_5yr/\")\n",
    "\n",
    "#iterate through all of the csv files\n",
    "for csv in individuals:\n",
    "    #only create paths with .csv included in the list\n",
    "    if '.csv' in csv:\n",
    "        #create a path with each csv\n",
    "        csv_path = \"data/sandp500/individual_stocks_5yr_TECHNICALS/\" + csv\n",
    "        #create a df with path\n",
    "        df = pd.read_csv(csv_path)\n",
    "        #find the original number of observations\n",
    "        orig_count = len(df.index)\n",
    "        try:\n",
    "            #clean the data by dropping the Acc/Dist_ROC_1 feature\n",
    "            df = df.drop('Acc/Dist_ROC_1', axis=1)\n",
    "            #drop all NaN values\n",
    "            df = df.dropna()\n",
    "            #save the cleaned df to csv in the individual_stocks_5yr_TECHNICALS_clean folder\n",
    "            df.to_csv(f\"data/sandp500/individual_stocks_5yr_TECHNICALS_clean/{csv}\")\n",
    "            #define the new count of the cleaned data \n",
    "            new_count = len(df.index)\n",
    "            #find the difference between the original count and the cleaned count\n",
    "            diff = orig_count - new_count\n",
    "            #add the difference, original_count and cleaned_count to the shape_diff dict\n",
    "            count_diff.update({f'{csv}':\n",
    "                               {'difference': diff, \n",
    "                               'original_count':orig_shape, \n",
    "                               'cleaned_count':new_shape}})\n",
    "        except:\n",
    "            print(f\"Technical cleaning failed on {csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a df from the shape_diff dictionary and transpose it\n",
    "obs_diff = pd.DataFrame(shape_diff).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_count</th>\n",
       "      <th>difference</th>\n",
       "      <th>original_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAL_data.csv</th>\n",
       "      <td>865</td>\n",
       "      <td>61</td>\n",
       "      <td>926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGN_data.csv</th>\n",
       "      <td>903</td>\n",
       "      <td>71</td>\n",
       "      <td>974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALLE_data.csv</th>\n",
       "      <td>869</td>\n",
       "      <td>71</td>\n",
       "      <td>940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AVGO_data.csv</th>\n",
       "      <td>326</td>\n",
       "      <td>61</td>\n",
       "      <td>387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BHF_data.csv</th>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BHGE_data.csv</th>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CCI_data.csv</th>\n",
       "      <td>598</td>\n",
       "      <td>71</td>\n",
       "      <td>669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CFG_data.csv</th>\n",
       "      <td>656</td>\n",
       "      <td>71</td>\n",
       "      <td>727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHTR_data.csv</th>\n",
       "      <td>251</td>\n",
       "      <td>61</td>\n",
       "      <td>312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CSRA_data.csv</th>\n",
       "      <td>367</td>\n",
       "      <td>71</td>\n",
       "      <td>438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DXC_data.csv</th>\n",
       "      <td>31</td>\n",
       "      <td>71</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EQIX_data.csv</th>\n",
       "      <td>582</td>\n",
       "      <td>61</td>\n",
       "      <td>643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EVHC_data.csv</th>\n",
       "      <td>103</td>\n",
       "      <td>71</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FTI_data.csv</th>\n",
       "      <td>74</td>\n",
       "      <td>71</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FTV_data.csv</th>\n",
       "      <td>221</td>\n",
       "      <td>73</td>\n",
       "      <td>294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLT_data.csv</th>\n",
       "      <td>852</td>\n",
       "      <td>71</td>\n",
       "      <td>923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HPE_data.csv</th>\n",
       "      <td>387</td>\n",
       "      <td>71</td>\n",
       "      <td>458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICE_data.csv</th>\n",
       "      <td>878</td>\n",
       "      <td>71</td>\n",
       "      <td>949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INFO_data.csv</th>\n",
       "      <td>733</td>\n",
       "      <td>61</td>\n",
       "      <td>794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IRM_data.csv</th>\n",
       "      <td>575</td>\n",
       "      <td>71</td>\n",
       "      <td>646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JCI_data.csv</th>\n",
       "      <td>618</td>\n",
       "      <td>71</td>\n",
       "      <td>689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KHC_data.csv</th>\n",
       "      <td>471</td>\n",
       "      <td>61</td>\n",
       "      <td>532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MDT_data.csv</th>\n",
       "      <td>571</td>\n",
       "      <td>71</td>\n",
       "      <td>642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MNST_data.csv</th>\n",
       "      <td>485</td>\n",
       "      <td>61</td>\n",
       "      <td>546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MYL_data.csv</th>\n",
       "      <td>558</td>\n",
       "      <td>61</td>\n",
       "      <td>619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NAVI_data.csv</th>\n",
       "      <td>776</td>\n",
       "      <td>61</td>\n",
       "      <td>837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLSN_data.csv</th>\n",
       "      <td>421</td>\n",
       "      <td>71</td>\n",
       "      <td>492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PNR_data.csv</th>\n",
       "      <td>735</td>\n",
       "      <td>71</td>\n",
       "      <td>806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRGO_data.csv</th>\n",
       "      <td>856</td>\n",
       "      <td>62</td>\n",
       "      <td>918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PYPL_data.csv</th>\n",
       "      <td>471</td>\n",
       "      <td>61</td>\n",
       "      <td>532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QRVO_data.csv</th>\n",
       "      <td>597</td>\n",
       "      <td>61</td>\n",
       "      <td>658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SYF_data.csv</th>\n",
       "      <td>694</td>\n",
       "      <td>71</td>\n",
       "      <td>765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UA_data.csv</th>\n",
       "      <td>278</td>\n",
       "      <td>71</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WBA_data.csv</th>\n",
       "      <td>598</td>\n",
       "      <td>61</td>\n",
       "      <td>659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WRK_data.csv</th>\n",
       "      <td>467</td>\n",
       "      <td>72</td>\n",
       "      <td>539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XL_data.csv</th>\n",
       "      <td>195</td>\n",
       "      <td>71</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               cleaned_count  difference  original_count\n",
       "AAL_data.csv             865          61             926\n",
       "AGN_data.csv             903          71             974\n",
       "ALLE_data.csv            869          71             940\n",
       "AVGO_data.csv            326          61             387\n",
       "BHF_data.csv               0          20              20\n",
       "BHGE_data.csv              0          28              28\n",
       "CCI_data.csv             598          71             669\n",
       "CFG_data.csv             656          71             727\n",
       "CHTR_data.csv            251          61             312\n",
       "CSRA_data.csv            367          71             438\n",
       "DXC_data.csv              31          71             102\n",
       "EQIX_data.csv            582          61             643\n",
       "EVHC_data.csv            103          71             174\n",
       "FTI_data.csv              74          71             145\n",
       "FTV_data.csv             221          73             294\n",
       "HLT_data.csv             852          71             923\n",
       "HPE_data.csv             387          71             458\n",
       "ICE_data.csv             878          71             949\n",
       "INFO_data.csv            733          61             794\n",
       "IRM_data.csv             575          71             646\n",
       "JCI_data.csv             618          71             689\n",
       "KHC_data.csv             471          61             532\n",
       "MDT_data.csv             571          71             642\n",
       "MNST_data.csv            485          61             546\n",
       "MYL_data.csv             558          61             619\n",
       "NAVI_data.csv            776          61             837\n",
       "NLSN_data.csv            421          71             492\n",
       "PNR_data.csv             735          71             806\n",
       "PRGO_data.csv            856          62             918\n",
       "PYPL_data.csv            471          61             532\n",
       "QRVO_data.csv            597          61             658\n",
       "SYF_data.csv             694          71             765\n",
       "UA_data.csv              278          71             349\n",
       "WBA_data.csv             598          61             659\n",
       "WRK_data.csv             467          72             539\n",
       "XL_data.csv              195          71             266"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show only the tickers where the original_count is less than 1000\n",
    "obs_diff[obs_diff['original_count']<1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cleaned_count     36\n",
       "difference        36\n",
       "original_count    36\n",
       "dtype: int64"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show how many tickers have less than 1000 observations \n",
    "obs_diff[obs_diff['original_count']<1000].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
